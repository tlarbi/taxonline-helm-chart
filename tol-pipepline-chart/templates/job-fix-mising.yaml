{{- if .Values.jobs.fixMissing.enabled }}
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ .Values.jobs.fixMissing.name }}
  namespace: {{ .Values.namespace }}
  labels:
    app: tol-pipeline
    job: fix-missing
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-weight": "0"
    "helm.sh/hook-delete-policy": before-hook-creation
spec:
  ttlSecondsAfterFinished: {{ .Values.jobs.fixMissing.ttlSecondsAfterFinished }}
  backoffLimit: 2
  template:
    metadata:
      labels:
        app: tol-pipeline
        job: fix-missing
    spec:
      restartPolicy: OnFailure
      containers:
        - name: fix-missing
          image: {{ .Values.image.repository }}:{{ .Values.image.tag }}
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          # Lance le script de fix des chunks manquants
          command: ["bash", "-c"]
          args:
            - |
              set -e
              echo "=== [1/3] Identify missing chunks ==="
              python3 -c "
              import json, urllib.request, os

              qdrant_url = os.environ['QDRANT_URL']
              collection = os.environ['QDRANT_COLLECTION']
              chunks_file = os.environ['CHUNKS_JSONL']
              min_chars = int(os.environ.get('MIN_CHARS', '50'))

              print(f'Fetching existing chunk IDs from Qdrant...', flush=True)
              url = f'{qdrant_url}/collections/{collection}/points/scroll'
              qdrant_ids = set()
              offset = None
              while True:
                  payload = {'limit': 250, 'with_payload': True, 'with_vector': False}
                  if offset:
                      payload['offset'] = offset
                  data = json.dumps(payload).encode()
                  req = urllib.request.Request(url, data=data,
                        headers={'Content-Type':'application/json'}, method='POST')
                  r = json.loads(urllib.request.urlopen(req, timeout=30).read())['result']
                  for p in r['points']:
                      qdrant_ids.add(p['payload'].get('chunk_id',''))
                  offset = r.get('next_page_offset')
                  if not offset:
                      break

              print(f'Qdrant: {len(qdrant_ids)} chunks', flush=True)

              missing = []
              with open(chunks_file) as f:
                  for line in f:
                      d = json.loads(line)
                      if d.get('chunk_id') and d['chunk_id'] not in qdrant_ids:
                          if len(d.get('text','')) >= min_chars:
                              missing.append(d)

              print(f'Missing: {len(missing)} chunks to index', flush=True)
              with open('/tmp/missing_chunks.jsonl', 'w') as f:
                  for d in missing:
                      f.write(json.dumps(d, ensure_ascii=False) + '\n')
              print('Saved to /tmp/missing_chunks.jsonl', flush=True)
              "

              MISSING=$(wc -l < /tmp/missing_chunks.jsonl)
              echo "=== [2/3] Indexing ${MISSING} missing chunks into Qdrant ==="
              if [ "$MISSING" -gt "0" ]; then
                CHUNKS_JSONL=/tmp/missing_chunks.jsonl python3 pipeline/03_index/index_qdrant.py
              else
                echo "Nothing to index in Qdrant."
              fi

              echo "=== [3/3] Sync missing chunks to OpenSearch ==="
              if [ "$MISSING" -gt "0" ]; then
                CHUNKS_JSONL=/tmp/missing_chunks.jsonl python3 pipeline/03_index/index_opensearch.py
              else
                echo "Nothing to index in OpenSearch."
              fi

              echo "=== DONE ==="
          env:
            {{- include "tol-pipeline.env" . | nindent 12 }}
          resources:
            {{- toYaml .Values.jobs.fixMissing.resources | nindent 12 }}
          volumeMounts:
            {{- include "tol-pipeline.volumeMount" . | nindent 12 }}
      volumes:
        {{- include "tol-pipeline.volume" . | nindent 8 }}
{{- end }}
